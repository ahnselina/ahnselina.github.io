---
title: raft协议解析
date: 2019-05-16 23:31:30
categories:
- 分布式
tags: [分布式, 一致性算法, raft]
---

![](https://z3.ax1x.com/2021/04/23/cOKAZq.jpg)

<!-- more -->

本文主要介绍raft一致性协议，在理解该协议的过程中，除了阅读原论文，还参考了很多参考资料中列出的文章，这些文章对我对raft的理解起了很大的帮助。在这些文章的基础上，遂按照自己的理解对相关资料做了整合更新。另外就是发现每次读原版论文和相关文章的时候都能读出新的东西，很是奇妙。PS:个人觉得这些经典的论文和资料可以多读几遍。

Raft实现了和Paxos相同的功能，它将一致性分解为多个子问题：Leader选举（Leader election）、日志同步（Log replication）、安全性（Safety）、日志压缩（Log compaction）、成员变更（Membership change）等。同时，Raft算法使用了更强的假设来减少了需要考虑的状态，使之变的易于理解和实现。  
本文通过Leader选举（Leader election）、日志同步（Log replication）、安全性（Safety）、日志压缩（Log compaction）、成员变更（Membership change）以及具体实例等方面来解析raft。

## 一致性(consensus)

在开始正式解析raft之前，我们看一下什么是一致性问题？举个简单的例子，现在的分布式存储系统中，为了保证数据的可靠性及服务的高可用性，一般一个数据都存三份，那么问题就来了，如何保证这三份复本的一致性，也就是要保证三个复本一样。此外，更广泛的一致性问题还指相互独立的节点之间如何达成一项决议的问题。分布式系统中，进行数据库事务提交(commit transaction)、Leader选举、序列号生成等都会遇到一致性问题。这个问题在我们的日常生活中也很常见，比如牌友怎么商定几点在哪打几圈麻将，有人可能会说，决定什么时候在哪搓麻将，4个人商量一下就ok，这不很简单吗？
但就这样看似简单的事情，分布式系统实现起来并不轻松，因为它面临着这些问题：

* **消息传递异步无序(asynchronous)**: 现实网络不是一个可靠的信道，存在消息延时、丢失，节点间消息传递做不到同步有序(synchronous)  
* **节点宕机(fail-stop)**: 节点持续宕机，不会恢复（这种情况对应我们存储系统中一般称为永久故障）
* **节点宕机恢复(fail-recover)**: 节点宕机一段时间后恢复，在分布式系统中最常见（这种情况对应我们存储系统中一般称为临时故障）
* **网络分化(network partition)**: 网络链路出现问题，将N个节点隔离成多个部分
* **拜占庭将军问题(byzantine failure)**: 节点或宕机或逻辑失败，甚至不按套路出牌抛出干扰决议的信息

假设一个具有N个节点的分布式系统，当其满足以下条件时，我们说这个系统满足一致性：

* **全认同(agreement)**: 所有N个节点都认同一个结果
* **值合法(validity)**: 该结果必须由N个节点中的节点提出
* **可结束(termination)**: 决议过程在一定时间内结束，不会无休止地进行下去

## raft协议概述

Raft将系统中的角色分为领导者（Leader）、跟从者（Follower）和候选人（Candidate）：

Leader：接受客户端请求，并向Follower同步请求日志，当日志同步到大多数节点上后告诉Follower提交日志。
Follower：接受并持久化Leader同步的日志，在Leader告之日志可以提交之后，提交日志。
Candidate：Leader选举过程中的临时角色。

Raft算法角色
在正常情况下，Raft要求系统在任意时刻最多只有一个Leader，正常工作期间只有Leader和Followers。

Raft算法角色状态转换如下：
![Raft算法角色状态转换](https://z3.ax1x.com/2021/04/23/cOKEd0.jpg)

Follower只响应其他服务器的请求。如果Follower超时没有收到Leader的消息，它会成为一个Candidate并且开始一次Leader选举。收到大多数服务器投票的Candidate会成为新的Leader。Leader在宕机之前会一直保持Leader的状态。

![](https://z3.ax1x.com/2021/04/23/cOKFLn.jpg)
Raft算法将时间分为一个个的任期（term），每一个term的开始都是Leader选举。在成功选举Leader之后，Leader会在整个term内管理整个集群。如果Leader选举失败，该term就会因为没有Leader而结束(这里的term相当于逻辑时钟，可以用term来判别一个消息是否过时)。Raft 保证了在一个给定的任期内，最多只有一个领导者。

## Leader election

Raft协议的每个副本都会处于三种状态之一：Leader、Follower、Candidate。

Leader：所有请求的处理者，Leader副本接受client的更新请求，本地处理后再同步到多个其他副本；
Follower：请求的被动更新者，从Leader接受更新请求，然后写入本地日志文件
Candidate：如果Follower副本在一段时间内没有收到Leader副本的心跳，则判断Leader可能已经故障，此时启动选主过程，此时副本会变成Candidate状态，直到选主结束。
时间被分为很多连续的随机长度的term，term有唯一的id。每个term一开始就进行选主：

* Follower将自己维护的current_term_id加1（即自增当前的任期号currentTerm）， 然后将自己的状态转成Candidate
* 给自己投票
* 重置选举超时计时器
* 发送RequestVoteRPC消息(带上current_term_id) 给 其它所有server

这个过程会有三种可能的结果：

1. 自己赢得了这次的选举，成了主。当收到了majority的投票后，状态切成Leader，并且定期给其它的所有server发心跳消息（不带log的AppendEntriesRPC）以告诉对方自己是current_term_id所标识的term的leader。每个term最多只有一个leader，**term id作为logical clock，在每个RPC消息中都会带上，用于检测过期的消息**。当一个server收到的RPC消息中的rpc_term_id比本地的current_term_id更大时，就更新current_term_id为rpc_term_id，并且如果当前state为leader或者candidate时，将自己的状态切成follower。如果rpc_term_id比本地的current_term_id更小，则拒绝这个RPC消息。
2. 其他的服务器成为主。如1所述，当Candidator在等待投票的过程中，收到了大于或者等于本地的current_term_id的声明对方是leader的AppendEntriesRPC时，则将自己的state切成follower，并且更新本地的current_term_id。
3. 没有选出主。当投票被瓜分，没有任何一个candidate收到了majority的vote时，没有leader被选出。这种情况下，每个candidate等待的投票的过程就超时了，接着candidates都会将本地的current_term_id再加1，发起RequestVoteRPC进行新一轮的leader election。

投票策略：

每个节点只会给每个term投一票，具体的是否同意和Safety有关，请参阅Safety（安全性）这一节。
当投票被瓜分后，所有的candidate同时超时，然后有可能进入新一轮的票数被瓜分，为了避免这个问题，Raft采用一种很简单的方法：每个Candidate的election timeout从150ms-300ms之间随机取，那么第一个超时的Candidate就可以发起新一轮的leader election，带着最大的term_id给其它所有server发送RequestVoteRPC消息，从而自己成为leader，然后给他们发送心跳消息以告诉他们自己是主。

## 日志复制

当Leader被选出来后，就可以接受客户端发来的请求了，每个请求包含一条需要被replicated state machines执行的命令。leader会把它作为一个log entry append到日志中，然后给其它的server发AppendEntriesRPC请求。当Leader确定一个log entry被safely replicated了（大多数副本已经将该命令写入日志当中），就apply这条log entry到状态机中然后返回结果给客户端。如果某个Follower宕机了或者运行的很慢，或者网络丢包了，则会一直给这个Follower发AppendEntriesRPC直到日志一致。

当一条日志是commited时，Leader才可以将它应用到状态机中。Raft保证一条commited的log entry已经持久化了并且会被所有的节点执行。

Raft日志复制保证如下两点：

* 如果不同日志中的两个条目有着相同的索引和任期号，则它们所存储的命令是相同的。
* 如果不同日志中的两个条目有着相同的索引和任期号，则它们之前的所有条目都是完全一样的。

![](https://z3.ax1x.com/2021/04/23/cOKVoV.jpg)

当一个新的Leader被选出来时，它的日志和其它的Follower的日志可能不一样，这个时候，就需要一个机制来保证日志的一致性。一个新leader产生时，集群状态可能如下：
![](https://z3.ax1x.com/2021/04/23/cOKuz4.jpg)
最上面这个是新leader,其上方的数字代表log index，a~f是follower，每个格子代表一条log entry，格子内的数字代表这个log entry是在哪个term上产生的。

新leader产生后，就以leader上的log为准。其它的follower要么少了数据比如b，要么多了数据，比如d，要么既少了又多了数据，比如f。

因此，需要有一种机制来让leader和follower对log达成一致，leader会为每个follower维护一个nextIndex，表示leader给各个follower发送的下一条log entry在log中的index，初始化为leader的最后一条log entry的下一个位置。leader给follower发送AppendEntriesRPC消息，带着(term_id, (nextIndex-1))， term_id即(nextIndex-1)这个槽位的log entry的term_id，follower接收到AppendEntriesRPC后，会从自己的log中找是不是存在这样的log entry，如果不存在，就给leader回复拒绝消息，然后leader则将nextIndex减1，再重复，知道AppendEntriesRPC消息被接收。

以leader和b为例：

初始化，nextIndex为11，leader给b发送AppendEntriesRPC(6,10)，b在自己log的10号槽位中没有找到term_id为6的log entry。则给leader回应一个拒绝消息。接着，leader将nextIndex减一，变成10，然后给b发送AppendEntriesRPC(6, 9)，b在自己log的9号槽位中同样没有找到term_id为6的log entry。循环下去，直到leader发送了AppendEntriesRPC(4,4)，b在自己log的槽位4中找到了term_id为4的log entry，便接收了消息。随后，leader就可以从槽位5开始给b推送日志了。

## Safety（安全性）

安全性主要解决什么样的节点可以成为主的问题，那么哪些follower有资格成为leader?

> **Raft保证被选为新leader的节点拥有所有已提交的log entry**，这与ViewStamped Replication不同，后者不需要这个保证，而是通过其他机制从follower拉取自己没有的提交的日志记录  

这个保证是在RequestVoteRPC阶段做的，candidate在发送RequestVoteRPC时，会带上自己的最后一条日志记录的term_id和index，其他节点收到消息时，如果发现自己的日志比RPC请求中携带的更新，拒绝投票。日志比较的原则是，如果本地的最后一条log entry的term id更大，则更新，如果term id一样大，则日志更多的更大(index更大)。

哪些日志记录被认为是commited?

> Leader只能推进commit index来提交当前term的已经复制到大多数服务器上的日志，旧term日志的提交要等到提交当前term的日志来间接提交（log index 小于 commit index的日志被间接提交）。

之所以要这样，是因为可能会出现已提交的日志又被覆盖的情况：
![](https://z3.ax1x.com/2021/04/23/cOKMQJ.jpg)

1. 在阶段a，term为2，S1是Leader，且S1写入日志（term, index）为(2, 2)，并且日志被同步写入了S2；  
2. 在阶段b，S1离线，触发一次新的选主，此时S5被选为新的Leader，此时系统term为3，且写入了日志（term, index）为（3， 2）;
3. S5尚未将日志推送到Followers变离线了，进而触发了一次新的选主，而之前离线的S1经过重新上线后被选中变成Leader，此时系统term为4，此时S1会将自己的日志同步到Followers，按照上图就是将日志（2， 2）同步到了S3，而此时由于该日志已经被同步到了多数节点（S1, S2, S3），因此，此时日志（2，2）可以被commit了（即更新到状态机）；
4. 在阶段d，S1又很不幸地下线了，系统触发一次选主，而S5有可能被选为新的Leader（这是因为S5可以满足作为主的一切条件：1. term = 3 > 2, 2. 最新的日志index为2，比大多数节点（如S2/S3/S4的日志都新），然后S5会将自己的日志更新到Followers，于是S2、S3中已经被提交的日志（2，2）被截断了，这是致命性的错误，因为一致性协议中不允许出现已经应用到状态机中的日志被截断。

为了避免这种致命错误，需要对协议进行一个微调：

> **只允许主节点提交包含当前term的日志**

针对上述情况就是：即使日志（2，2）已经被大多数节点（S1、S2、S3）确认了，但是它不能被Commit，因为它是来自之前term(2)的日志，直到S1在当前term（4）产生的日志（4， 3）被大多数Follower确认，S1方可Commit（4，3）这条日志，当然，根据Raft定义，（4，3）之前的所有日志也会被Commit。此时即使S1再下线，重新选主时S5不可能成为Leader，因为它没有包含大多数节点已经拥有的日志（4，3）。

总结就是为了安全性考虑，raft中对要成为主的节点增加了以下两点限制：

* 拥有最新的已提交的log entry的Follower才有资格成为Leader
* Leader只能推进commit index来提交当前term的已经复制到大多数服务器上的日志，leader不能直接提交旧的term的日志，即使旧term的日志已经复制到大多数服务器上了，旧term日志的提交要等到提交当前term的日志来间接提交（log index 小于 commit index的日志被间接提交）。

## 集群拓扑变化(Cluser Membership 变更)

前面文章阐述的一个基础是加速集群配置是固定的（即参与到一致性算法中的服务器的集合是固定的）。然而在实际中配置变更却经常是必要，比如替换掉故障的机器或者从三副本变更到六副本。当然我们可以通过下线整个集群、更新配置文件、然后重启集群来达到配置变更的目的，但是这会使集群在做变更操作期间不可用。此外，如果这个过程是手动操作的，那么还会有操作失误的风险。为了避免这些问题，raft自动化了整个配置变更过程。  
为了确保配置变更机制安全，那就必须保证在转换的过程中，在同一term内，没有出现两个leader的情况。

> 成员变更也是一个分布式一致性问题，既所有服务器对新成员达成一致。但是成员变更又有其特殊性，因为在成员变更的一致性达成的过程中，参与投票的进程会发生变化。    
> 如果将成员变更当成一般的一致性问题，直接向Leader发送成员变更请求，Leader复制成员变更日志，达成多数派之后提交，各服务器提交成员变更日志后从旧成员配置（Cold）切换到新成员配置（Cnew）。  
> 因为各个服务器提交成员变更日志的时刻可能不同，造成各个服务器从旧成员配置（Cold）切换到新成员配置（Cnew）的时刻不同。  
> 成员变更不能影响服务的可用性，但是成员变更过程的某一时刻，可能出现在Cold和Cnew中同时存在两个不相交的多数派，进而可能选出两个Leader，形成不同的决议，破坏安全性。

下图是成员变更的某一时刻Cold和Cnew中同时存在两个不相交的多数派，这种情况下就有可能出现server2认为自己是leader，而server3也认为自己是主的情况。（server2看到集群配置是server1,server2,server3,然后server1和server2投票给server2，这是一个多数派；而server3看得集群配置是server1,server2,server3,server4,server5，然后server3,server4,server5投票给server3，这是第二个多数派，所以这种情况下会出现两个leader）
![](https://pic3.zhimg.com/v2-c8e4ead21f6f2e9d40361717739519c6_b.jpg)
由于成员变更的这一特殊性，成员变更不能当成一般的一致性问题去解决。为了解决这一问题，Raft提出了两阶段的成员变更方法。集群先从旧成员配置Cold切换到一个过渡成员配置，称为共同一致（joint consensus），共同一致是旧成员配置Cold和新成员配置Cnew的组合Cold U Cnew，一旦共同一致Cold U Cnew被提交，系统再切换到新成员配置Cnew。
![](https://pic3.zhimg.com/v2-6b85a141cd131aa129a4e70d060f37be_b.jpg)

**Raft两阶段成员变更过程如下**：  

1. Leader收到成员变更请求从Cold切成Cnew；  
2. Leader在本地生成一个新的log entry，其内容是Cold∪Cnew，代表当前时刻新旧成员配置共存，写入本地日志，同时将该log entry复制至Cold∪Cnew中的所有副本。在此之后新的日志同步需要保证得到Cold和Cnew两个多数派的确认；  
3. Follower收到Cold∪Cnew的log entry后更新本地日志，并且此时就以该配置作为自己的成员配置；  
4. 如果Cold和Cnew中的两个多数派确认了Cold U Cnew这条日志，Leader就提交这条log entry；  
5. 接下来Leader生成一条新的log entry，其内容是新成员配置Cnew，同样将该log entry写入本地日志，同时复制到Follower上；  
6. Follower收到新成员配置Cnew后，将其写入日志，并且从此刻起，就以该配置作为自己的成员配置，并且如果发现自己不在Cnew这个成员配置中会自动退出；  
7. Leader收到Cnew的多数派确认后，表示成员变更成功，后续的日志只要得到Cnew多数派确认即可。Leader给客户端回复成员变更执行成功。

**异常分析**：  

* 如果Leader的Cold U Cnew尚未推送到Follower，Leader就挂了，此后选出的新Leader并不包含这条日志，此时新Leader依然使用Cold作为自己的成员配置。
* 如果Leader的Cold U Cnew推送到大部分的Follower后就挂了，此后选出的新Leader可能是Cold也可能是Cnew中的某个Follower。
* 如果Leader在推送Cnew配置的过程中挂了，那么同样，新选出来的Leader可能是Cold也可能是Cnew中的某一个，此后客户端继续执行一次改变配置的命令即可。
* 如果大多数的Follower确认了Cnew这个消息后，那么接下来即使Leader挂了，新选出来的Leader肯定位于Cnew中。  

两阶段成员变更比较通用且容易理解，但是实现比较复杂，同时两阶段的变更协议也会在一定程度上影响变更过程中的服务可用性，因此我们期望增强成员变更的限制，以简化操作流程。  
两阶段成员变更，之所以分为两个阶段，是因为对Cold与Cnew的关系没有做任何假设，为了避免Cold和Cnew各自形成不相交的多数派选出两个Leader，才引入了两阶段方案。  
如果增强成员变更的限制，假设Cold与Cnew任意的多数派交集不为空，这两个成员配置就无法各自形成多数派，那么成员变更方案就可能简化为一阶段。  
那么如何限制Cold与Cnew，使之任意的多数派交集不为空呢？方法就是每次成员变更只允许增加或删除一个成员。  

**可从数学上严格证明，只要每次只允许增加或删除一个成员，Cold与Cnew不可能形成两个不相交的多数派。**  

一阶段成员变更：

* 成员变更限制每次只能增加或删除一个成员（如果要变更多个成员，连续变更多次）。
* 成员变更由Leader发起，Cnew得到多数派确认后，返回客户端成员变更成功。
* 一次成员变更成功前不允许开始下一次成员变更，因此新任Leader在开始提供服务前要将自己本地保存的最新成员配置重新投票形成多数派确认。
* Leader只要开始同步新成员配置，即可开始使用新的成员配置进行日志同步。

## Log Compaction（日志压缩）

在实际的系统中，不能让日志无限增长，否则系统重启时需要花很长的时间进行回放，从而影响可用性。Raft采用对整个系统进行snapshot来解决，snapshot之前的日志都可以丢弃。

每个副本独立的对自己的系统状态进行snapshot，并且只能对已经提交的日志记录进行snapshot。

Snapshot中包含以下内容：

* 日志元数据。最后一条已提交的 log entry的 log index和term。这两个值在snapshot之后的第一条log entry的AppendEntries RPC的完整性检查的时候会被用上。
* 系统当前状态   
  ![](https://z3.ax1x.com/2021/04/23/cOKlLR.jpg) 
  当Leader要发给某个日志落后太多的Follower的log entry被丢弃，Leader会将snapshot发给Follower。或者当新加进一台机器时，也会发送snapshot给它。发送snapshot使用InstalledSnapshot RPC（RPC细节参见八、Raft算法总结）。

做snapshot既不要做的太频繁，否则消耗磁盘带宽， 也不要做的太不频繁，否则一旦节点重启需要回放大量日志，影响可用性。推荐当日志达到某个固定的大小做一次snapshot。

做一次snapshot可能耗时过长，会影响正常日志同步。可以通过使用copy-on-write技术避免snapshot过程影响正常日志同步。

## Raft与Paxos

Raft 其实就是 Multi-Paxos 的一个变种，Raft 通过简化 Multi-Paxos 的模型，实现了一种更容易让人理解的共识算法，它们两者都能够对一系列连续的问题达成一致。
Raft 在 Multi-Paxos 的基础之上做了两个限制，首先是 Raft 中追加日志的操作必须是连续的，而 Multi-Paxos 中追加日志的操作是并发的，但是对于节点内部的状态机来说两者都是有序的，第二就是 Raft 对 Leader 选举的条件做了限制，只有拥有最新、最全日志的节点才能够当选 Leader，但是 Multi-Paxos 由于任意节点都可以写日志，所以在选择 Leader 上也没有什么限制，只是在选择 Leader 之后需要将 Leader 中的日志补全。

## 实例分析

略  
可参考[动画图解raft](http://thesecretlivesofdata.com/raft/) 

## 参考资料：

[raft论文](https://raft.github.io/raft.pdf)  
[raft博士论文](https://web.stanford.edu/~ouster/cgi-bin/papers/OngaroPhD.pdf)
[The Raft Consensus Algorithm](https://raft.github.io/)  
[动画图解raft](http://thesecretlivesofdata.com/raft/)  
[raft论文翻译](https://github.com/maemual/raft-zh_cn/blob/master/raft-zh_cn.md)  
[Raft算法原理](https://www.codedump.info/post/20180921-raft/)  
[Raft协议详解](https://zhuanlan.zhihu.com/p/27207160)  
[Raft算法详解](https://zhuanlan.zhihu.com/p/32052223)  
[别再怀疑自己的智商了，Raft协议本来就不好理解](https://zhuanlan.zhihu.com/p/36547283)  
[raft优化可参考](https://pingcap.com/blog-cn/optimizing-raft-in-tikv/)
[MIT课程有关raft](https://zhuanlan.zhihu.com/p/49460319)  
